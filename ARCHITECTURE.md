# Architecture Documentation

## System Overview

This PDF Chatbot implements a **Retrieval-Augmented Generation (RAG)** system that combines document retrieval with language model generation.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PDF CHATBOT SYSTEM                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                            USER INTERFACE
                         (Streamlit Web App)
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼             â–¼             â–¼
              Upload PDF    Enter API Key   Ask Question
                    â”‚             â”‚             â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ PDF Processing â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                   â–¼                   â–¼
   [Extract Text]   [Split Text]        [Create Docs]
   (PyPDF2)         (250-2000 chars)     (LangChain)
        â”‚                   â”‚                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Embedding      â”‚
                    â”‚ Generation     â”‚
                    â”‚(HuggingFace)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ FAISS Vector   â”‚
                    â”‚ Store Index    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        RETRIEVAL PHASE                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        User Question
            â”‚
            â–¼
    [Query Embedding]
    (Same HF Model)
            â”‚
            â–¼
    [FAISS Similarity]
    (Find k=4 closest)
            â”‚
            â–¼
    [Retrieved Chunks]
    (Context docs)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      GENERATION PHASE                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Retrieved Context  +  Question
            â”‚                 â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼
         [Format into Prompt]
                     â”‚
                     â–¼
       [System Prompt Template]
                     â”‚
                     â–¼
        [Groq LLM - Llama 3.3]
                     â”‚
                     â–¼
            [Generated Answer]
                     â”‚
                     â–¼
       [Display to User + Save]
```

## Data Flow

### 1. Document Preparation (One-time)

```python
PDF File
  â””â”€â†’ PyPDF2.PdfReader()
       â””â”€â†’ Extract text from all pages
           â””â”€â†’ RecursiveCharacterTextSplitter()
               â””â”€â†’ Create overlapping chunks (1000 chars, 200 overlap)
                   â””â”€â†’ Document() objects (LangChain)
                       â””â”€â†’ HuggingFaceEmbeddings.embed_documents()
                           â””â”€â†’ 384-dimensional vectors
                               â””â”€â†’ FAISS.from_documents()
                                   â””â”€â†’ Indexed Vector Store
```

**Why overlapping chunks?**
- Prevents questions from spanning chunk boundaries
- Ensures context continuity
- Example: If a sentence extends from chunk 2â†’3, both get retrieved

### 2. Inference (Per question)

```
User Question ("What is the candidate's experience?")
  â””â”€â†’ LCEL Chain processes:
      1. Question â†’ HF Embeddings â†’ 384-dim vector
      2. FAISS retrieves k=4 most similar chunks
      3. Format chunks: "\n\n".join() â†’ single context string
      4. Create prompt:
         â”œâ”€ System instructions
         â”œâ”€ Retrieved context
         â””â”€ Question
      5. Groq LLM generates response
      6. Extract .content from response object
      7. Display to user + save to history
```

## Component Details

### Text Processing Pipeline

**RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)**

```
Original Text: "Lorem ipsum dolor sit amet... [very long] ...consectetur adipiscing elit"

Split Logic:
1. Try to split by "\n\n" (paragraphs)
2. If too long, try splitting by "\n" (lines)
3. If still too long, try splitting by " " (words)
4. Finally split by character if necessary

Result:
Chunk 1: [0-1000 chars]
Chunk 2: [800-1800] â† overlaps 200 chars with Chunk 1
Chunk 3: [1600-2600] â† overlaps 200 chars with Chunk 2
...
```

### Embedding Model

**HuggingFace "all-MiniLM-L6-v2"**
- Dimension: 384
- Max token length: 256 words
- Speed: ~50 documents/sec (CPU)
- Size: 22MB
- Similarity metric: Cosine distance

Example:
```
"Senior Developer with 5 years Python experience"
  â””â”€â†’ Tokenizer
      â””â”€â†’ [49, 202, 15, 890, ...] (tokens)
          â””â”€â†’ Embedding Layer
              â””â”€â†’ [-0.23, 0.45, -0.12, ..., 0.98] (384 values)
```

### Vector Store (FAISS)

**Facebook AI Similarity Search**

```
Indexed Documents:
â”œâ”€ Doc1: "5 years Python..." â†’ [-0.23, 0.45, ...]
â”œâ”€ Doc2: "Java specialist..." â†’ [0.12, -0.34, ...]
â”œâ”€ Doc3: "Python Django..." â†’ [-0.25, 0.43, ...]
â””â”€ Doc4: "C++ embedded..." â†’ [0.98, -0.12, ...]

Query: "Python experience" â†’ [-0.24, 0.44, ...]

Similarity Scores:
â”œâ”€ Doc1: 0.98 âœ“ (most similar)
â”œâ”€ Doc3: 0.96 âœ“
â”œâ”€ Doc2: 0.62
â””â”€ Doc4: 0.15

Retrieval (k=4): [Doc1, Doc3, Doc2, Doc4]
```

### LLM Integration (Groq)

**Model: Llama 3.3 70B Versatile**
- Inference speed: ~100 tokens/sec
- Context window: 8192 tokens
- Training data: Cut-off unknown (trained on public data)

Prompt format:
```
[System] You are a precise assistant...
[Context] Doc1: "..." \n\n Doc2: "..."
[Question] "What is the experience?"
[Assistant] "The candidate has..."
```

### Session State Management

Streamlit's `session_state` ensures data persists across widget interactions:

```python
# First visit: All initialized
st.session_state.messages = []
st.session_state.chain = None

# User uploads PDF â†’ build_chain() â†’ chain created
st.session_state.chain = RAGChain(...)

# User asks question â†’ chain reused
for _ in range(10_questions):
    response = st.session_state.chain.invoke(question)
    # No rebuild needed!

# Different file uploaded â†’ rebuild triggered
if st.session_state.current_file != new_file.name:
    st.session_state.chain = build_chain(new_text, api_key)
    st.session_state.messages = []  # Clear history
```

## Error Handling

### Common Failure Points

1. **PDF Extraction**
   - Empty pages â†’ check `if extracted:`
   - Image-only PDFs â†’ would fail (OCR not implemented)
   - Corrupted files â†’ PyPDF2 raises exception

2. **Embedding Generation**
   - Network error â†’ offline check needed
   - Token limit exceeded â†’ split long texts
   - OOM on large documents â†’ reduce chunk_size

3. **FAISS Indexing**
   - Empty documents â†’ validates at runtime
   - Memory constraints â†’ should warn user

4. **LLM Generation**
   - API key invalid â†’ 401 error
   - Rate limiting â†’ exponential backoff (user handles)
   - Context too long â†’ truncates gracefully

## Performance Characteristics

### Time Breakdown (typical 50-page PDF)

| Phase | Time | Notes |
|-------|------|-------|
| Extract Text | 0.5s | I/O bound |
| Split into chunks | 0.1s | CPU bound |
| Generate embeddings | 3-5s | Depends on CPU |
| Build FAISS index | 0.5s | Memory bound |
| **Per Question** | | |
| Produce query embedding | 0.05s | Cached model |
| FAISS retrieval | 0.01s | Vec DB lookup |
| LLM inference | 2-4s | Network + compute |
| **Total** | 5-10s | Per question |

### Memory Usage

| Component | Size |
|-----------|------|
| Embedding model | 22 MB |
| FAISS index (50 pages) | ~5-10 MB |
| Chat history (100 msgs) | ~1 MB |
| **Total** | ~30 MB user |

### Scalability Limits

| Metric | Limit | Reason |
|--------|-------|--------|
| PDF Size | ~200 MB | RAM embedding |
| Chunks | ~10K | FAISS perf |
| Users | Single | No backend |
| Questions/session | Unlimited | History grows |

## Optimization Opportunities

### Quick Wins
1. âœ… Use `faiss-gpu` for 10x speedup
2. âœ… Cache embeddings to disk
3. âœ… Lazy-load model on first use
4. âš ï¸ Batch questions in notebook

### Future Improvements
- [ ] Multi-user backend (Flask/FastAPI)
- [ ] Database persistence
- [ ] Citation/source tracking
- [ ] Semantic caching
- [ ] Reranking step (ColBERT)

## Security Considerations

âš ï¸ **Current**: No user authentication
- API keys stored in .env (local only)
- No document encryption
- No access controls

ğŸ”’ **For Production**:
- Use secrets manager (AWS Secrets Manager)
- Encrypt PDFs at rest
- Rate limiting per user
- Audit logging
- HTTPS only
